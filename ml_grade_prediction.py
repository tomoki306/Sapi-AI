"""
æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æˆç¸¾äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  (scikit-learn)
éå»ã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°†æ¥ã®æˆç¸¾ã‚’äºˆæ¸¬ã™ã‚‹é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
from logger import log_info, log_error
import json
import pickle
import os


class GradePredictionML:
    """æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æˆç¸¾äºˆæ¸¬ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.models = {
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'linear_regression': LinearRegression(),
            'ridge': Ridge(alpha=1.0)
        }
        self.scaler = StandardScaler()
        self.best_model = None
        self.best_model_name = None
        self.feature_names = []
        
    def prepare_features(self, grades_data):
        """
        æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
        
        Args:
            grades_data: æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ç‰¹å¾´é‡ã®DataFrame
        """
        if not grades_data or len(grades_data) < 3:
            return None
        
        # DataFrameã«å¤‰æ›
        df = pd.DataFrame(grades_data)
        df['date'] = pd.to_datetime(df['date'], format='mixed')
        df = df.sort_values('date')
        
        features = []
        
        for i in range(len(df)):
            if i < 2:  # æœ€ä½3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦
                continue
            
            # éå»ã®ãƒ‡ãƒ¼ã‚¿
            past_data = df.iloc[:i]
            current_row = df.iloc[i]
            
            # ç‰¹å¾´é‡ã®è¨ˆç®—
            feature_dict = {
                # åŸºæœ¬çµ±è¨ˆé‡
                'mean_grade': past_data['grade'].mean(),
                'std_grade': past_data['grade'].std(),
                'min_grade': past_data['grade'].min(),
                'max_grade': past_data['grade'].max(),
                'median_grade': past_data['grade'].median(),
                
                # ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´
                'recent_avg_3': past_data['grade'].tail(3).mean() if len(past_data) >= 3 else past_data['grade'].mean(),
                'recent_avg_5': past_data['grade'].tail(5).mean() if len(past_data) >= 5 else past_data['grade'].mean(),
                'trend_slope': self._calculate_trend(past_data['grade'].values),
                
                # å¤‰å‹•æ€§
                'coefficient_variation': (past_data['grade'].std() / past_data['grade'].mean()) if past_data['grade'].mean() > 0 else 0,
                
                # ã‚«ã‚¦ãƒ³ãƒˆ
                'num_tests': len(past_data[past_data['type'] == 'ãƒ†ã‚¹ãƒˆ']),
                'num_assignments': len(past_data[past_data['type'] == 'èª²é¡Œ']),
                
                # é‡ã¿ä»˜ãå¹³å‡
                'weighted_avg': (past_data['grade'] * past_data['weight']).sum() / past_data['weight'].sum() if past_data['weight'].sum() > 0 else past_data['grade'].mean(),
                
                # æ™‚ç³»åˆ—ç‰¹å¾´
                'days_since_first': (current_row['date'] - past_data['date'].min()) / np.timedelta64(1, 'D'),
                'avg_days_between': self._calculate_avg_interval(past_data['date'].values),
                
                # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆå®Ÿéš›ã®æˆç¸¾ï¼‰
                'target': current_row['grade']
            }
            
            features.append(feature_dict)
        
        return pd.DataFrame(features)
    
    def _calculate_trend(self, grades):
        """æˆç¸¾ã®ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå‚¾ãï¼‰ã‚’è¨ˆç®—"""
        if len(grades) < 2:
            return 0
        x = np.arange(len(grades))
        try:
            slope = np.polyfit(x, grades, 1)[0]
            return slope
        except:
            return 0
    
    def _calculate_avg_interval(self, dates):
        """å¹³å‡æ—¥æ•°é–“éš”ã‚’è¨ˆç®—"""
        if len(dates) < 2:
            return 0
        intervals = []
        for i in range(len(dates)-1):
            diff = dates[i+1] - dates[i]
            # numpy.timedelta64 ã‚’æ—¥æ•°ã«å¤‰æ›
            days = diff / np.timedelta64(1, 'D')
            intervals.append(days)
        return np.mean(intervals) if intervals else 0
    
    def train(self, grades_data, subject_name):
        """
        ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        
        Args:
            grades_data: æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            subject_name: ç§‘ç›®å
            
        Returns:
            è¨“ç·´çµæœã®è¾æ›¸
        """
        try:
            # ç‰¹å¾´é‡ã®æº–å‚™
            df = self.prepare_features(grades_data)
            
            if df is None or len(df) < 5:
                return {
                    'success': False,
                    'error': 'ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½5ä»¶ã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰'
                }
            
            # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®åˆ†é›¢
            X = df.drop('target', axis=1)
            y = df['target']
            self.feature_names = X.columns.tolist()
            
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’èª¿æ•´
            if len(df) < 10:
                test_size = 0.2  # å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
            elif len(df) < 20:
                test_size = 0.25  # ä¸­ç¨‹åº¦
            else:
                test_size = 0.3  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿
            
            # ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42, shuffle=True
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã®è¨ºæ–­æƒ…å ±
            diagnosis = {
                'total_samples': len(df),
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'target_mean': float(y.mean()),
                'target_std': float(y.std()),
                'target_min': float(y.min()),
                'target_max': float(y.max()),
                'has_nan': X.isnull().any().any() or y.isnull().any(),
                'feature_count': len(self.feature_names)
            }
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            # å„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
            results = {}
            best_score = -float('inf')
            
            for name, model in self.models.items():
                # è¨“ç·´
                model.fit(X_train_scaled, y_train)
                
                # äºˆæ¸¬
                y_pred = model.predict(X_test_scaled)
                
                # è©•ä¾¡æŒ‡æ¨™
                mse = mean_squared_error(y_test, y_pred)
                rmse = np.sqrt(mse)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                cv_scores = cross_val_score(
                    model, X_train_scaled, y_train, 
                    cv=min(5, len(X_train)), 
                    scoring='r2'
                )
                
                results[name] = {
                    'rmse': rmse,
                    'mae': mae,
                    'r2': r2,
                    'cv_mean': cv_scores.mean(),
                    'cv_std': cv_scores.std()
                }
                
                # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠï¼ˆR2ã‚¹ã‚³ã‚¢ã§åˆ¤æ–­ï¼‰
                if r2 > best_score:
                    best_score = r2
                    self.best_model = model
                    self.best_model_name = name
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            self._save_model(subject_name)
            
            log_info(f"æˆç¸¾äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã—ãŸ: {subject_name} (ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {self.best_model_name})", "ML_TRAINING")
            
            return {
                'success': True,
                'results': results,
                'best_model': self.best_model_name,
                'best_score': best_score,
                'feature_importance': self._get_feature_importance(),
                'diagnosis': diagnosis  # è¨ºæ–­æƒ…å ±ã‚’è¿½åŠ 
            }
            
        except Exception as e:
            log_error(e, f"ML_TRAINING_ERROR: {subject_name}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def predict_next_grade(self, grades_data, num_predictions=3):
        """
        æ¬¡ã®æˆç¸¾ã‚’äºˆæ¸¬
        
        Args:
            grades_data: æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            num_predictions: äºˆæ¸¬ã™ã‚‹æ•°
            
        Returns:
            äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆ
        """
        if not self.best_model:
            return None
        
        try:
            df = pd.DataFrame(grades_data)
            df['date'] = pd.to_datetime(df['date'], format='mixed')
            df = df.sort_values('date')
            
            predictions = []
            
            for i in range(num_predictions):
                # ç‰¹å¾´é‡ã®è¨ˆç®—
                feature_dict = {
                    'mean_grade': df['grade'].mean(),
                    'std_grade': df['grade'].std(),
                    'min_grade': df['grade'].min(),
                    'max_grade': df['grade'].max(),
                    'median_grade': df['grade'].median(),
                    'recent_avg_3': df['grade'].tail(3).mean() if len(df) >= 3 else df['grade'].mean(),
                    'recent_avg_5': df['grade'].tail(5).mean() if len(df) >= 5 else df['grade'].mean(),
                    'trend_slope': self._calculate_trend(df['grade'].values),
                    'coefficient_variation': (df['grade'].std() / df['grade'].mean()) if df['grade'].mean() > 0 else 0,
                    'num_tests': len(df[df['type'] == 'ãƒ†ã‚¹ãƒˆ']),
                    'num_assignments': len(df[df['type'] == 'èª²é¡Œ']),
                    'weighted_avg': (df['grade'] * df['weight']).sum() / df['weight'].sum() if df['weight'].sum() > 0 else df['grade'].mean(),
                    'days_since_first': (pd.Timestamp(datetime.now()) - df['date'].min()) / np.timedelta64(1, 'D'),
                    'avg_days_between': self._calculate_avg_interval(df['date'].values)
                }
                
                # ç‰¹å¾´é‡ã‚’DataFrameã«å¤‰æ›
                X_pred = pd.DataFrame([feature_dict])
                
                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                X_pred_scaled = self.scaler.transform(X_pred)
                
                # äºˆæ¸¬
                predicted_grade = self.best_model.predict(X_pred_scaled)[0]
                
                # äºˆæ¸¬å€¤ã‚’0-100ã®ç¯„å›²ã«åˆ¶é™
                predicted_grade = max(0, min(100, predicted_grade))
                
                predictions.append({
                    'prediction_num': i + 1,
                    'predicted_grade': round(predicted_grade, 1),
                    'confidence_interval': self._calculate_confidence_interval(predicted_grade)
                })
                
                # æ¬¡ã®äºˆæ¸¬ã®ãŸã‚ã«äºˆæ¸¬å€¤ã‚’ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                next_date = df['date'].max() + timedelta(days=int(feature_dict['avg_days_between']))
                new_row = pd.DataFrame([{
                    'date': next_date,
                    'type': 'ãƒ†ã‚¹ãƒˆ',
                    'grade': predicted_grade,
                    'weight': df['weight'].mean()
                }])
                df = pd.concat([df, new_row], ignore_index=True)
            
            return predictions
            
        except Exception as e:
            log_error(e, "ML_PREDICTION_ERROR")
            return None
    
    def _calculate_confidence_interval(self, prediction, confidence=0.95):
        """ä¿¡é ¼åŒºé–“ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # æ¨™æº–èª¤å·®ã®æ¨å®š
        std_error = 5.0  # ä»®ã®å€¤ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ã™ã¹ãï¼‰
        margin = 1.96 * std_error  # 95%ä¿¡é ¼åŒºé–“
        
        return {
            'lower': max(0, round(prediction - margin, 1)),
            'upper': min(100, round(prediction + margin, 1))
        }
    
    def _get_feature_importance(self):
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—"""
        if not self.best_model or not hasattr(self.best_model, 'feature_importances_'):
            return None
        
        importance = self.best_model.feature_importances_
        return dict(zip(self.feature_names, importance))
    
    def _save_model(self, subject_name):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        try:
            model_dir = 'ml_models'
            os.makedirs(model_dir, exist_ok=True)
            
            model_path = os.path.join(model_dir, f'{subject_name}_model.pkl')
            
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': self.best_model,
                    'scaler': self.scaler,
                    'model_name': self.best_model_name,
                    'feature_names': self.feature_names
                }, f)
            
            log_info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}", "ML_SAVE")
            
        except Exception as e:
            log_error(e, "ML_SAVE_ERROR")
    
    def load_model(self, subject_name):
        """ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            model_path = os.path.join('ml_models', f'{subject_name}_model.pkl')
            
            if not os.path.exists(model_path):
                return False
            
            with open(model_path, 'rb') as f:
                data = pickle.load(f)
            
            self.best_model = data['model']
            self.scaler = data['scaler']
            self.best_model_name = data['model_name']
            self.feature_names = data['feature_names']
            
            log_info(f"ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {model_path}", "ML_LOAD")
            return True
            
        except Exception as e:
            log_error(e, "ML_LOAD_ERROR")
            return False


def display_ml_grade_prediction():
    """æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹æˆç¸¾äºˆæ¸¬UIã®è¡¨ç¤º"""
    st.header("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹é«˜ç²¾åº¦æˆç¸¾äºˆæ¸¬")
    
    st.info("""
    **scikit-learn ã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ **
    - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãªã©è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒ
    - è‡ªå‹•çš„ã«æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    - éå»ã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°†æ¥ã®æˆç¸¾ã‚’äºˆæ¸¬
    """)
    
    # æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    if 'grades' not in st.session_state or not st.session_state.grades:
        st.warning("æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšæˆç¸¾ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # ç§‘ç›®ã®é¸æŠ
    subjects = list(st.session_state.grades.keys())
    selected_subject = st.selectbox("äºˆæ¸¬ã™ã‚‹ç§‘ç›®ã‚’é¸æŠ", subjects)
    
    if not selected_subject:
        return
    
    grades_data = st.session_state.grades[selected_subject]
    
    if len(grades_data) < 5:
        st.warning(f"âš ï¸ {selected_subject}ã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆç¾åœ¨{len(grades_data)}ä»¶ã€æœ€ä½5ä»¶å¿…è¦ï¼‰")
        return
    
    # ã‚¿ãƒ–ã§æ©Ÿèƒ½ã‚’åˆ†å‰²
    tab1, tab2, tab3 = st.tabs(["ğŸ“Š ãƒ¢ãƒ‡ãƒ«è¨“ç·´", "ğŸ”® æˆç¸¾äºˆæ¸¬", "ğŸ“ˆ åˆ†æãƒ»å¯è¦–åŒ–"])
    
    with tab1:
        st.subheader("ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´")
        
        st.write(f"**ãƒ‡ãƒ¼ã‚¿æ•°**: {len(grades_data)}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        df_check = pd.DataFrame(grades_data)
        df_check['grade'] = pd.to_numeric(df_check['grade'], errors='coerce')
        
        grade_std = df_check['grade'].std()
        grade_mean = df_check['grade'].mean()
        coefficient_of_variation = (grade_std / grade_mean) if grade_mean > 0 else 0
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã®è¡¨ç¤º
        with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("å¹³å‡ç‚¹", f"{grade_mean:.1f}")
            with col2:
                st.metric("æ¨™æº–åå·®", f"{grade_std:.1f}")
            with col3:
                cv_status = "å®‰å®š" if coefficient_of_variation < 0.3 else "å¤‰å‹•å¤§"
                st.metric("å¤‰å‹•ä¿‚æ•°", f"{coefficient_of_variation:.2f}", cv_status)
            
            # ãƒ‡ãƒ¼ã‚¿é‡ã®è©•ä¾¡
            if len(grades_data) < 10:
                st.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ã§ã™ï¼ˆç¾åœ¨{len(grades_data)}ä»¶ï¼‰ã€‚ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€10ä»¶ä»¥ä¸Šã®è¨˜éŒ²ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
            elif len(grades_data) < 20:
                st.info(f"ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ã¯ååˆ†ã§ã™ãŒã€20ä»¶ä»¥ä¸Šã‚ã‚‹ã¨ã‚ˆã‚Šé«˜ç²¾åº¦ã«ãªã‚Šã¾ã™ã€‚")
            else:
                st.success(f"âœ… ååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡ã§ã™ï¼é«˜ç²¾åº¦ãªäºˆæ¸¬ãŒæœŸå¾…ã§ãã¾ã™ã€‚")
            
            # å¤‰å‹•ã®è©•ä¾¡
            if coefficient_of_variation > 0.4:
                st.warning("âš ï¸ æˆç¸¾ã®å¤‰å‹•ãŒå¤§ãã„ã§ã™ã€‚äºˆæ¸¬ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            elif coefficient_of_variation > 0.3:
                st.info("ğŸ’¡ æˆç¸¾ã«é©åº¦ãªå¤‰å‹•ãŒã‚ã‚Šã¾ã™ã€‚")
            else:
                st.success("âœ… æˆç¸¾ãŒå®‰å®šã—ã¦ã„ã¾ã™ã€‚")
        
        if st.button("ğŸš€ ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´", type="primary"):
            with st.spinner("ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­..."):
                ml_predictor = GradePredictionML()
                result = ml_predictor.train(grades_data, selected_subject)
                
                if result['success']:
                    best_score = result['best_score']
                    
                    # RÂ²ã‚¹ã‚³ã‚¢ã®è©•ä¾¡ã¨èª¬æ˜
                    if best_score >= 0.8:
                        score_emoji = "â­â­â­â­â­"
                        score_text = "éå¸¸ã«å„ªç§€"
                        score_color = "green"
                    elif best_score >= 0.6:
                        score_emoji = "â­â­â­â­"
                        score_text = "è‰¯å¥½"
                        score_color = "green"
                    elif best_score >= 0.4:
                        score_emoji = "â­â­â­"
                        score_text = "æ™®é€š"
                        score_color = "orange"
                    elif best_score >= 0:
                        score_emoji = "â­â­"
                        score_text = "æ”¹å–„ãŒå¿…è¦"
                        score_color = "orange"
                    else:
                        score_emoji = "â­"
                        score_text = "è¦æ”¹å–„ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¾ãŸã¯ä¸è¦å‰‡ï¼‰"
                        score_color = "red"
                    
                    st.success(f"âœ… è¨“ç·´å®Œäº†ï¼ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: **{result['best_model']}**")
                    
                    # RÂ²ã‚¹ã‚³ã‚¢ã®èª¬æ˜ãƒœãƒƒã‚¯ã‚¹
                    if best_score < 0:
                        # è¨ºæ–­æƒ…å ±ã®å–å¾—
                        diag = result.get('diagnosis', {})
                        
                        # å…·ä½“çš„ãªå•é¡Œç‚¹ã‚’è¨ºæ–­
                        issues = []
                        recommendations = []
                        
                        # ãƒ‡ãƒ¼ã‚¿é‡ã®ç¢ºèª
                        if diag.get('total_samples', 0) < 15:
                            issues.append(f"âœ— ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆ{diag.get('total_samples', 0)}ä»¶ â†’ æ¨å¥¨: 20ä»¶ä»¥ä¸Šï¼‰")
                            recommendations.append("ğŸ“Š ã‚ˆã‚Šå¤šãã®æˆç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„")
                        
                        # å¤‰å‹•ã®ç¢ºèª
                        target_std = diag.get('target_std', 0)
                        target_mean = diag.get('target_mean', 0)
                        if target_mean > 0:
                            cv = target_std / target_mean
                            if cv > 0.4:
                                issues.append(f"âœ— æˆç¸¾ã®å¤‰å‹•ãŒå¤§ãã„ï¼ˆå¤‰å‹•ä¿‚æ•°: {cv:.2f}ï¼‰")
                                recommendations.append("ğŸ“ˆ å­¦ç¿’æ–¹æ³•ã‚’å®‰å®šã•ã›ã¦ã€æˆç¸¾ã®å¤‰å‹•ã‚’æŠ‘ãˆã¦ãã ã•ã„")
                        
                        # æˆç¸¾ç¯„å›²ã®ç¢ºèª
                        target_range = diag.get('target_max', 100) - diag.get('target_min', 0)
                        if target_range < 20:
                            issues.append(f"âœ— æˆç¸¾ã®ç¯„å›²ãŒç‹­ã„ï¼ˆ{target_range:.1f}ç‚¹ï¼‰")
                            recommendations.append("ğŸ’¡ ãƒ‡ãƒ¼ã‚¿ãŒä¼¼ãŸå€¤ã°ã‹ã‚Šã ã¨äºˆæ¸¬ãŒé›£ã—ããªã‚Šã¾ã™")
                        
                        # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«æ•°ã®ç¢ºèª
                        if diag.get('test_samples', 0) < 3:
                            issues.append(f"âœ— ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ï¼ˆ{diag.get('test_samples', 0)}ä»¶ï¼‰")
                            recommendations.append("ğŸ“ å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„")
                        
                        # è¨ºæ–­çµæœã®è¡¨ç¤º
                        st.error(f"""
                        ### {score_emoji} RÂ²ã‚¹ã‚³ã‚¢: {best_score:.3f} ({score_text})
                        
                        **âš ï¸ è² ã®RÂ²ã‚¹ã‚³ã‚¢ã«ã¤ã„ã¦:**
                        RÂ²ã‚¹ã‚³ã‚¢ãŒè² ã®å€¤ï¼ˆ{best_score:.3f}ï¼‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒã€Œå¹³å‡å€¤ã‚’ä½¿ã†ã‚ˆã‚Šæ‚ªã„ã€ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚
                        
                        **ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œç‚¹:**
                        {chr(10).join(issues) if issues else '- ç‰¹å®šã®å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'}
                        
                        **ğŸ’¡ å…·ä½“çš„ãªæ”¹å–„æ–¹æ³•:**
                        {chr(10).join(recommendations) if recommendations else '1. ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„'}
                        
                        **ğŸ“Š ãƒ‡ãƒ¼ã‚¿è¨ºæ–­:**
                        - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {diag.get('train_samples', 0)}ä»¶
                        - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {diag.get('test_samples', 0)}ä»¶
                        - æˆç¸¾å¹³å‡: {diag.get('target_mean', 0):.1f}ç‚¹
                        - æˆç¸¾ç¯„å›²: {diag.get('target_min', 0):.1f}ã€œ{diag.get('target_max', 100):.1f}ç‚¹
                        
                        **ç¾çŠ¶ã§ã®ä½¿ç”¨:**
                        äºˆæ¸¬æ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã™ãŒã€ç²¾åº¦ãŒä½ã„ãŸã‚å‚è€ƒç¨‹åº¦ã«ã—ã¦ãã ã•ã„ã€‚
                        """)
                    elif best_score < 0.4:
                        st.warning(f"""
                        ### {score_emoji} RÂ²ã‚¹ã‚³ã‚¢: {best_score:.3f} ({score_text})
                        
                        **ğŸ“Š ã‚¹ã‚³ã‚¢ã®æ„å‘³:**
                        ãƒ¢ãƒ‡ãƒ«ã¯å‹•ä½œã—ã¾ã™ãŒã€äºˆæ¸¬ç²¾åº¦ã¯ä½ã‚ã§ã™ã€‚
                        
                        **æ”¹å–„æ–¹æ³•:**
                        - ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ï¼ˆç¾åœ¨: {len(grades_data)}ä»¶ â†’ æ¨å¥¨: 15ä»¶ä»¥ä¸Šï¼‰
                        - ãƒ‡ãƒ¼ã‚¿ã®å¤‰å‹•ã‚’ç¢ºèª
                        """)
                    else:
                        st.info(f"""
                        ### {score_emoji} RÂ²ã‚¹ã‚³ã‚¢: {best_score:.3f} ({score_text})
                        
                        **ğŸ“Š ã‚¹ã‚³ã‚¢ã®æ„å‘³:**
                        RÂ²ã‚¹ã‚³ã‚¢ã¯ã€Œãƒ¢ãƒ‡ãƒ«ãŒã©ã‚Œã ã‘ãƒ‡ãƒ¼ã‚¿ã‚’èª¬æ˜ã§ãã‚‹ã‹ã€ã‚’ç¤ºã—ã¾ã™ï¼ˆ0ã€œ1ãŒæ­£å¸¸ç¯„å›²ï¼‰ã€‚
                        
                        - **1.0**: å®Œç’§ãªäºˆæ¸¬
                        - **0.8ä»¥ä¸Š**: éå¸¸ã«å„ªç§€ â­â­â­â­â­
                        - **0.6-0.8**: è‰¯å¥½ â­â­â­â­
                        - **0.4-0.6**: æ™®é€š â­â­â­
                        - **0.4æœªæº€**: æ”¹å–„ãŒå¿…è¦ â­â­
                        """)
                    
                    # ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ
                    st.subheader("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ")
                    
                    comparison_data = []
                    for model_name, metrics in result['results'].items():
                        comparison_data.append({
                            'ãƒ¢ãƒ‡ãƒ«': model_name,
                            'RÂ² ã‚¹ã‚³ã‚¢': f"{metrics['r2']:.3f}",
                            'RMSE': f"{metrics['rmse']:.2f}",
                            'MAE': f"{metrics['mae']:.2f}",
                            'CVå¹³å‡': f"{metrics['cv_mean']:.3f}",
                            'CVæ¨™æº–åå·®': f"{metrics['cv_std']:.3f}"
                        })
                    
                    df_comparison = pd.DataFrame(comparison_data)
                    st.dataframe(df_comparison, use_container_width=True)
                    
                    # ç‰¹å¾´é‡ã®é‡è¦åº¦
                    if result.get('feature_importance'):
                        st.subheader("ğŸ¯ ç‰¹å¾´é‡ã®é‡è¦åº¦")
                        importance_df = pd.DataFrame(
                            list(result['feature_importance'].items()),
                            columns=['ç‰¹å¾´é‡', 'é‡è¦åº¦']
                        ).sort_values('é‡è¦åº¦', ascending=False)
                        
                        fig = px.bar(
                            importance_df.head(10),
                            x='é‡è¦åº¦',
                            y='ç‰¹å¾´é‡',
                            orientation='h',
                            title='ä¸Šä½10ã®é‡è¦ãªç‰¹å¾´é‡'
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                else:
                    st.error(f"âŒ è¨“ç·´å¤±æ•—: {result['error']}")
    
    with tab2:
        st.subheader("æˆç¸¾äºˆæ¸¬")
        
        # ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        ml_predictor = GradePredictionML()
        if not ml_predictor.load_model(selected_subject):
            st.warning("âš ï¸ ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãšã€Œãƒ¢ãƒ‡ãƒ«è¨“ç·´ã€ã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„ã€‚")
            return
        
        st.success(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {ml_predictor.best_model_name}")
        
        num_predictions = st.slider("äºˆæ¸¬ã™ã‚‹å›æ•°", 1, 10, 3)
        
        if st.button("ğŸ”® äºˆæ¸¬ã‚’å®Ÿè¡Œ", type="primary"):
            with st.spinner("äºˆæ¸¬ä¸­..."):
                predictions = ml_predictor.predict_next_grade(grades_data, num_predictions)
                
                if predictions:
                    st.subheader("ğŸ“Š äºˆæ¸¬çµæœ")
                    
                    for pred in predictions:
                        col1, col2 = st.columns([1, 2])
                        
                        with col1:
                            st.metric(
                                f"äºˆæ¸¬ {pred['prediction_num']}",
                                f"{pred['predicted_grade']:.1f}ç‚¹"
                            )
                        
                        with col2:
                            ci = pred['confidence_interval']
                            st.write(f"**95%ä¿¡é ¼åŒºé–“**: {ci['lower']:.1f} ~ {ci['upper']:.1f}ç‚¹")
                            
                            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼é¢¨ã®å¯è¦–åŒ–
                            progress = pred['predicted_grade'] / 100
                            st.progress(progress)
                    
                    # äºˆæ¸¬ã®å¯è¦–åŒ–
                    st.subheader("ğŸ“ˆ äºˆæ¸¬ã®å¯è¦–åŒ–")
                    
                    # éå»ã®ãƒ‡ãƒ¼ã‚¿
                    df_history = pd.DataFrame(grades_data)
                    df_history['date'] = pd.to_datetime(df_history['date'], format='mixed')
                    df_history = df_history.sort_values('date')
                    df_history['type_label'] = 'å®Ÿç¸¾'
                    
                    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
                    last_date = df_history['date'].max()
                    avg_interval = ml_predictor._calculate_avg_interval(df_history['date'].values)
                    
                    pred_data = []
                    for i, pred in enumerate(predictions):
                        pred_date = last_date + timedelta(days=int(avg_interval * (i + 1)))
                        pred_data.append({
                            'date': pred_date,
                            'grade': pred['predicted_grade'],
                            'type_label': 'äºˆæ¸¬'
                        })
                    
                    df_pred = pd.DataFrame(pred_data)
                    
                    # ã‚°ãƒ©ãƒ•ä½œæˆ
                    fig = go.Figure()
                    
                    # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿
                    fig.add_trace(go.Scatter(
                        x=df_history['date'],
                        y=df_history['grade'],
                        mode='lines+markers',
                        name='å®Ÿç¸¾',
                        line=dict(color='blue', width=2),
                        marker=dict(size=8)
                    ))
                    
                    # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
                    fig.add_trace(go.Scatter(
                        x=df_pred['date'],
                        y=df_pred['grade'],
                        mode='lines+markers',
                        name='äºˆæ¸¬',
                        line=dict(color='red', width=2, dash='dash'),
                        marker=dict(size=8, symbol='star')
                    ))
                    
                    fig.update_layout(
                        title='æˆç¸¾ã®æ¨ç§»ã¨äºˆæ¸¬',
                        xaxis_title='æ—¥ä»˜',
                        yaxis_title='æˆç¸¾',
                        yaxis=dict(range=[0, 100]),
                        hovermode='x unified'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                else:
                    st.error("äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    with tab3:
        st.subheader("åˆ†æãƒ»å¯è¦–åŒ–")
        
        # åŸºæœ¬çµ±è¨ˆ
        df = pd.DataFrame(grades_data)
        df['date'] = pd.to_datetime(df['date'], format='mixed')
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("å¹³å‡ç‚¹", f"{df['grade'].mean():.1f}")
        with col2:
            st.metric("æ¨™æº–åå·®", f"{df['grade'].std():.1f}")
        with col3:
            st.metric("æœ€é«˜ç‚¹", f"{df['grade'].max():.0f}")
        with col4:
            st.metric("æœ€ä½ç‚¹", f"{df['grade'].min():.0f}")
        
        # æˆç¸¾åˆ†å¸ƒ
        st.subheader("ğŸ“Š æˆç¸¾åˆ†å¸ƒ")
        fig_hist = px.histogram(
            df,
            x='grade',
            nbins=20,
            title='æˆç¸¾ã®åˆ†å¸ƒ',
            labels={'grade': 'æˆç¸¾', 'count': 'åº¦æ•°'}
        )
        st.plotly_chart(fig_hist, use_container_width=True)
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        st.subheader("ğŸ“ˆ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ")
        ml_predictor = GradePredictionML()
        trend_slope = ml_predictor._calculate_trend(df['grade'].values)
        
        if trend_slope > 0.5:
            st.success(f"ğŸ“ˆ ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå‚¾ã: {trend_slope:.2f}ï¼‰- æˆç¸¾ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ï¼")
        elif trend_slope < -0.5:
            st.warning(f"ğŸ“‰ ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå‚¾ã: {trend_slope:.2f}ï¼‰- æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
        else:
            st.info(f"â¡ï¸ å®‰å®šï¼ˆå‚¾ã: {trend_slope:.2f}ï¼‰- å®‰å®šã—ãŸæˆç¸¾ã‚’ç¶­æŒã—ã¦ã„ã¾ã™")


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    display_ml_grade_prediction()
