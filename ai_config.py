"""
AIæ©Ÿèƒ½ã®å…±é€šè¨­å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
Azure OpenAIæ¥ç¶šè¨­å®šã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã€å®šæ•°å®šç¾©
"""
import os
import streamlit as st
from openai import AzureOpenAI
from dotenv import load_dotenv

# .env ã‹ã‚‰èª­ã¿è¾¼ã¿
load_dotenv()

# Azure OpenAIè¨­å®š
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_KEY = os.getenv("AZURE_OPENAI_KEY")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2025-04-01-preview")
MODEL_NAME = os.getenv("AZURE_OPENAI_MODEL", "gpt-5-mini")

# ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™è¨­å®šï¼ˆGPT-5-miniæœ€é©åŒ–ï¼‰
DEFAULT_MAX_COMPLETION_TOKENS = 5000
YOUTUBE_QUIZ_MAX_COMPLETION_TOKENS = 10000

# reasoning_effortè¨­å®šï¼ˆGPT-5-miniç”¨ï¼‰
# ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ãŒé‡è¦
# 
# minimal: é€Ÿåº¦æœ€å„ªå…ˆãƒ»å˜ç´”ã‚¿ã‚¹ã‚¯å‘ã‘
#   - ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å®šæƒ…å ±ã‚’å–ã‚Šå‡ºã™ï¼‰
#   - ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¤‰æ›ï¼ˆJSONå¤‰æ›ã€æ•´å½¢ãªã©ï¼‰
#   - ç°¡å˜ãªåˆ†é¡ï¼ˆpositive/negative/neutralãªã©ï¼‰
#   - çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆã®æ›¸ãæ›ãˆ
#   - é€Ÿåº¦ãŒé‡è¦ãªãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
#
# low: æ¯”è¼ƒçš„å˜ç´”ãªæ¨è«–ã‚¿ã‚¹ã‚¯å‘ã‘
#   - æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
#   - ç°¡å˜ãªæ¨è«–ã‚’å«ã‚€ã‚¿ã‚¹ã‚¯
#   - ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å°‘ãªã„ã€é€Ÿåº¦: é€Ÿã„
#
# medium: ä¸€èˆ¬çš„ãªã‚¿ã‚¹ã‚¯å‘ã‘ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
#   - è¤‡é›‘ãªåˆ†æã‚„è¨ˆç”»ã‚¿ã‚¹ã‚¯
#   - åŒ…æ‹¬çš„ãªè©•ä¾¡ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
#   - ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: ä¸­ç¨‹åº¦ã€é€Ÿåº¦: ä¸­ç¨‹åº¦
#
# high: è¤‡é›‘ãªå¤šæ®µéšã‚¿ã‚¹ã‚¯å‘ã‘ï¼ˆæœ€ã‚‚é…ã„ï¼‰
#   - å¤§è¦æ¨¡ãªæˆ¦ç•¥ç«‹æ¡ˆ
#   - ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã®è¤‡é›‘ãªæ¨è«–
#   - ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: å¤šã„ã€é€Ÿåº¦: é…ã„
#
# ğŸ’¡ é¸æŠæŒ‡é‡: ã‚³ã‚¹ãƒˆå‰Šæ¸›å„ªå…ˆ â†’ minimal/lowã€å“è³ªå„ªå…ˆ â†’ medium/high
DEFAULT_REASONING_EFFORT = "minimal"  # é€Ÿåº¦æœ€å„ªå…ˆã®å˜ç´”ã‚¿ã‚¹ã‚¯ç”¨
LOW_REASONING_EFFORT = "low"          # æ¯”è¼ƒçš„å˜ç´”ãªæ¨è«–ã‚¿ã‚¹ã‚¯ç”¨
COMPLEX_REASONING_EFFORT = "medium"   # ä¸€èˆ¬çš„ãªè¤‡é›‘ã‚¿ã‚¹ã‚¯ç”¨

# æ¥ç¶šç¢ºèª
if not AZURE_OPENAI_ENDPOINT or not AZURE_OPENAI_KEY:
    st.error("AZURE_OPENAI_ENDPOINT / AZURE_OPENAI_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.stop()

# Azure OpenAI ClientåˆæœŸåŒ–
client = AzureOpenAI(
    azure_endpoint=AZURE_OPENAI_ENDPOINT,
    api_key=AZURE_OPENAI_KEY,
    api_version=AZURE_OPENAI_API_VERSION,
)

VISION_ENDPOINT = AZURE_OPENAI_ENDPOINT
VISION_KEY = AZURE_OPENAI_KEY


def determine_reasoning_effort(input_length: int, task_complexity: str = "medium") -> str:
    """
    ãƒ‡ãƒ¼ã‚¿é‡ã¨ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦æœ€é©ãªreasoning_effortã‚’æ±ºå®š
    
    Args:
        input_length: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ï¼ˆæ–‡å­—æ•°ï¼‰
        task_complexity: ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã• ("simple", "medium", "complex")
        
    Returns:
        æœ€é©ãªreasoning_effort ("minimal", "low", "medium", "high")
        
    Note:
        - simple: ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã€åˆ†é¡ã€çŸ­ã„æ›¸ãæ›ãˆ
        - medium: åˆ†æã€è©•ä¾¡ã€æ§‹é€ åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
        - complex: å¤šæ®µéšæ¨è«–ã€æˆ¦ç•¥ç«‹æ¡ˆã€åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    # ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«ã‚ˆã‚‹åŸºæœ¬è¨­å®š
    if task_complexity == "simple":
        base_effort = "minimal"
        threshold_low = 1000
        threshold_medium = 5000
    elif task_complexity == "complex":
        base_effort = "medium"
        threshold_low = 500
        threshold_medium = 2000
    else:  # medium
        base_effort = "low"
        threshold_low = 500
        threshold_medium = 2000
    
    # ãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ã¦èª¿æ•´
    if input_length < threshold_low:
        if task_complexity == "simple":
            return "minimal"
        elif task_complexity == "complex":
            return "low"
        else:
            return "minimal"
    elif input_length < threshold_medium:
        if task_complexity == "simple":
            return "minimal"
        elif task_complexity == "complex":
            return "medium"
        else:
            return "low"
    else:
        if task_complexity == "simple":
            return "low"
        elif task_complexity == "complex":
            return "medium"
        else:
            return "medium"


def get_ai_response(prompt, system_content="æ•™è‚²æ”¯æ´AIã§ã™ã€‚", max_tokens=DEFAULT_MAX_COMPLETION_TOKENS, reasoning_effort=DEFAULT_REASONING_EFFORT):
    """GPT-5-miniç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸæ±ç”¨AIå¿œç­”é–¢æ•°
    
    Args:
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_content: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5000ï¼‰
        reasoning_effort: æ¨è«–åŠªåŠ›ãƒ¬ãƒ™ãƒ«ï¼ˆ"minimal", "low", "medium", "high"ï¼‰
            - minimal: è»½é‡ã‚¿ã‚¹ã‚¯ï¼ˆæŠ½å‡ºã€åˆ†é¡ã€çŸ­ã„æ›¸ãæ›ãˆï¼‰- é€Ÿåº¦å„ªå…ˆ
            - low: ä½ã„æ¨è«–åŠªåŠ›
            - medium: æ¨™æº–ï¼ˆè¤‡é›‘ãªã‚¿ã‚¹ã‚¯ï¼‰
            - high: é«˜ã„æ¨è«–åŠªåŠ›ï¼ˆãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¹ã‚¯ï¼‰
    
    Note:
        GPT-5æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã¯temperatureã€top_pã€presence_penaltyã€
        frequency_penaltyãªã©ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
        max_completion_tokensã¯æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³+å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆè¨ˆã§ã™ã€‚
    """
    try:
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºæ™‚ã®ã¿è¡¨ç¤ºï¼‰
        if st.session_state.get('debug_mode', False):
            st.write(f"ğŸ“Š ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}æ–‡å­—")
            st.write(f"ğŸ¯ max_tokens: {max_tokens}")
            st.write(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")
            st.write(f"ğŸ§  reasoning_effort: {reasoning_effort}")
        
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt}
            ],
            max_completion_tokens=max_tokens,
            reasoning_effort=reasoning_effort  # GPT-5-miniç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
        )
        
        # ğŸ” è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        st.write("### ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±")
        st.write(f"**finish_reason**: `{response.choices[0].finish_reason}`")
        st.write(f"**model**: `{response.model}`")
        
        # ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¡¨ç¤ºï¼ˆGPT-5æ¨è«–ãƒ¢ãƒ‡ãƒ«ã§ã¯é‡è¦ï¼‰
        if hasattr(response, 'usage') and response.usage:
            st.write("**ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°**:")
            st.write(f"  - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: `{response.usage.prompt_tokens}` tokens")
            st.write(f"  - å®Œäº†: `{response.usage.completion_tokens}` tokens")
            st.write(f"  - åˆè¨ˆ: `{response.usage.total_tokens}` tokens")
            
            # GPT-5æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ç¤º
            if hasattr(response.usage, 'completion_tokens_details'):
                details = response.usage.completion_tokens_details
                if hasattr(details, 'reasoning_tokens') and details.reasoning_tokens:
                    st.write(f"  - ğŸ§  æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³: `{details.reasoning_tokens}` tokens")
                    st.write(f"  - ğŸ“ å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: `{response.usage.completion_tokens - details.reasoning_tokens}` tokens")
        
        # refusalã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆGPT-5ã§ã¯é‡è¦ï¼‰
        if hasattr(response.choices[0].message, 'refusal') and response.choices[0].message.refusal:
            st.error(f"ğŸš« ãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã‚’æ‹’å¦ã—ã¾ã—ãŸ: {response.choices[0].message.refusal}")
            return f"ãƒ¢ãƒ‡ãƒ«ãŒå¿œç­”ã‚’æ‹’å¦ã—ã¾ã—ãŸ: {response.choices[0].message.refusal}"
        
        result = response.choices[0].message.content
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·ã‚’ãƒã‚§ãƒƒã‚¯
        content_length = len(result) if result else 0
        st.write(f"**content length**: `{content_length}` æ–‡å­—")
        
        # finish_reasonã®è©³ç´°ãƒã‚§ãƒƒã‚¯
        if response.choices[0].finish_reason == "content_filter":
            st.error("ğŸš« Azure Content Safety Filterã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ")
            st.info("ğŸ’¡ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¾ãŸã¯ç”Ÿæˆå†…å®¹ãŒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼ã«é•åã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã‚Šå¿œç­”ãŒåˆ¶é™ã•ã‚Œã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
        
        if response.choices[0].finish_reason == "length":
            st.warning(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆ{max_tokens}ï¼‰ã«é”ã—ã¾ã—ãŸã€‚max_tokensã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚")
            if result:
                st.info("âš ï¸ å¿œç­”ãŒé€”ä¸­ã§åˆ‡ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # ç©ºã®å¿œç­”ãƒã‚§ãƒƒã‚¯
        if not result or result.strip() == "":
            st.error(f"âš ï¸ ç©ºã®å¿œç­”ãŒè¿”ã•ã‚Œã¾ã—ãŸï¼ˆfinish_reason: `{response.choices[0].finish_reason}`ï¼‰")
            
            # å®Œå…¨ãªresponseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ‡ãƒãƒƒã‚°è¡¨ç¤º
            with st.expander("ğŸ” å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"):
                st.json(response.model_dump())
            
            return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸Šè¨˜ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        
        st.success("âœ… æ­£å¸¸ã«å¿œç­”ã‚’å—ä¿¡ã—ã¾ã—ãŸ")
        return result
        
    except Exception as e:
        from openai import (
            APIError, 
            RateLimitError, 
            APIConnectionError,
            AuthenticationError,
            BadRequestError,
            InternalServerError
        )
        from logger import log_error
        
        error_msg = str(e)
        error_type = type(e).__name__
        
        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        log_error(e, "AI_API_ERROR", details={
            "model": MODEL_NAME,
            "reasoning_effort": reasoning_effort,
            "max_tokens": max_tokens,
            "prompt_length": len(prompt)
        })
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if isinstance(e, RateLimitError):
            st.error("â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ")
            st.info("ğŸ’¡ ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€Azure ãƒãƒ¼ã‚¿ãƒ«ã§ã‚¯ã‚©ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return "ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        
        elif isinstance(e, APIConnectionError):
            st.error("ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            st.info("ğŸ’¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚VPNã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹ã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
            return "æ¥ç¶šã‚¨ãƒ©ãƒ¼ã€‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        
        elif isinstance(e, AuthenticationError):
            st.error("ï¿½ èªè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            st.info("ğŸ’¡ .env ãƒ•ã‚¡ã‚¤ãƒ«ã® AZURE_OPENAI_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return "èªè¨¼ã‚¨ãƒ©ãƒ¼ã€‚APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        
        elif isinstance(e, BadRequestError):
            st.error("âš ï¸ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            st.info(f"ğŸ’¡ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™: {error_msg}")
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹å ´åˆã®ãƒ’ãƒ³ãƒˆ
            if len(prompt) > 10000:
                st.warning("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å†…å®¹ã‚’çŸ­ç¸®ã—ã¦ãã ã•ã„ã€‚")
            return f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {error_msg}"
        
        elif isinstance(e, InternalServerError):
            st.error("ğŸ”´ Azure OpenAI ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼")
            st.info("ğŸ’¡ Azureå´ã§ä¸€æ™‚çš„ãªå•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
            return "ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚"
        
        elif isinstance(e, APIError):
            st.error(f"âŒ APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡: {error_type}")
            return f"APIã‚¨ãƒ©ãƒ¼: {error_msg}"
        
        else:
            # ãã®ä»–ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
            st.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}")
            st.error(f"ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡: {error_type}")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å±•é–‹å¯èƒ½ãªå½¢ã§è¡¨ç¤º
            with st.expander("ğŸ” è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ï¼ˆé–‹ç™ºè€…å‘ã‘ï¼‰"):
                st.code(error_msg)
                import traceback
                st.code(traceback.format_exc())
            
            return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_type}"


def call_api_with_retry(func, max_retries=3, initial_wait=1, backoff_factor=2):
    """APIå‘¼ã³å‡ºã—ã‚’ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§å®Ÿè¡Œã™ã‚‹æ±ç”¨é–¢æ•°
    
    Args:
        func: å‘¼ã³å‡ºã™é–¢æ•°ï¼ˆlambdaç­‰ï¼‰
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
        initial_wait: åˆå›ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        backoff_factor: ãƒªãƒˆãƒ©ã‚¤ã”ã¨ã«å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™å€ç‡
    
    Returns:
        funcã®å®Ÿè¡Œçµæœ
    
    Raises:
        æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã€ä¾‹å¤–ã‚’å†é€å‡º
    """
    import time
    
    wait_time = initial_wait
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                st.warning(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆ{attempt + 1}/{max_retries}å›ç›®ï¼‰: {str(e)}")
                st.info(f"â³ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(wait_time)
                wait_time *= backoff_factor
            else:
                st.error(f"âŒ {max_retries}å›ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã—ãŸãŒå¤±æ•—ã—ã¾ã—ãŸ")
    
    # æœ€å¾Œã®ãƒªãƒˆãƒ©ã‚¤ã§ã‚‚å¤±æ•—ã—ãŸå ´åˆ
    raise last_exception


def test_azure_openai_connection():
    """Azure OpenAI (GPT-5-mini) æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹é–¢æ•°"""
    st.write("### Azure OpenAI (GPT-5-mini) æ¥ç¶šãƒ†ã‚¹ãƒˆ")
    
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "user", "content": "ã“ã‚“ã«ã¡ã¯ã€‚æ¥ç¶šãƒ†ã‚¹ãƒˆã§ã™ã€‚çŸ­ãè¿”ç­”ã—ã¦ãã ã•ã„ã€‚"}
            ],
            max_completion_tokens=100
        )
        
        result = response.choices[0].message.content
        st.success(f"Azure OpenAIæ¥ç¶šæˆåŠŸ (ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME})")
        st.write(f"å¿œç­”: {result}")
        return True
        
    except Exception as e:
        st.error("Azure OpenAIæ¥ç¶šå¤±æ•—")
        st.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
        return False
